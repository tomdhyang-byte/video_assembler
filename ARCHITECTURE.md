
# 🏗️ AutoVideoMaker V10 系統架構與原理解析

本文檔詳細說明 AutoVideoMaker V10 的核心處理流程，拆解每個對影片品質有重大影響的關鍵步驟。

## 🧠 核心理念 (Core Philosophy)
1.  **Single Source of Truth (SSOT)**：所有音訊依賴以 `avatar_full.mp4` 為基準，不重新混音，確保對嘴 100% 同步。
2.  **Frame-Perfect Precision**：所有時間計算皆轉換為「幀 (Frame)」單位，消除浮點數累積誤差 (Drift)。
3.  **Content-Aware Alignment**：不依賴檔名順序，而是透過「聽」音訊內容 (Audio Fingerprinting) 來決定 Slide 出現的位置。

---

## 流程詳解

### 階段一：精準字幕生成 (`generate_subtitles.py`)

這個階段的目標是產出「逐字精確」且「斷句優美」的字幕檔。

| 步驟 | 邏輯與技術 | 關鍵作用 |
| :--- | :--- | :--- |
| **0. 音軌提取** | 從 `avatar_full.mp4` 直接提取音訊。 | **防止影音不同步**。這是唯一的與嘴型同步的音訊源。 |
| **1. 語音辨識** | 呼叫 **OpenAI Whisper API** (Whisper-v2 large)。設定 `timestamp_granularities=['word']`。 | 取得每一個「字 (Word)」的開始與結束時間 (Start/End Timestamp)。 |
| **2. 強制對齊 (Force Alignment)** | 使用 **Dynamic Time Warping (DTW)** 演算法，將 Whisper 辨識出的字，與用戶提供的「正確逐字稿」進行比對。 | **修正錯字與時間校正**。Whisper 偶爾會聽錯，但對齊後可以同時擁有「正確文字」與「精確時間」。 |
| **3. 智慧分段 (GPT-4o-mini)** | 使用 **LLM** 進行語意分析，將長文切分為適合閱讀的短句 (每行限制 18 字)。 | **提升閱讀體驗**。避免字幕在不該斷的地方換行（如：專有名詞被切開）。 |
| **4. 時間軸合成** | 將 GPT 分好的「句子」與 Step 2 的「字級時間戳」重新映射 (Mapping)。 | 產出最終的 SRT/ASS 字幕檔，時間精確到 0.01 秒。 |

### 🔍 深入解析：Force Alignment 運作邏輯

**Force Alignment** 是解決「語音辨識不夠準確」與「正確逐字稿沒有時間戳」這兩難局面的關鍵技術。

**運作原理 (Dynamic Time Warping)：**
想像兩條字串：
1.  **Whisper 聽到的 (有時間，可能有錯字)**：`["今天", "天起", "很好"]` (對應時間 T1, T2, T3)
2.  **正確逐字稿 (無時間，文字正確)**：`["今天", "天氣", "很好"]`

演算法會計算兩者的「相似度矩陣」，並找到一條「代價最小的路徑」來將它們配對：

| Whisper (有時間) | 正確稿 (無時間) | 配對結果 | 最終產出 |
| :--- | :--- | :--- | :--- |
| "今天" (0.0s-0.5s) | "今天" | ✅ Match | **"今天" (0.0s-0.5s)** |
| "天起" (0.5s-1.0s) | "天氣" | ⚠️ Mismatch (但在最佳路徑上) | **"天氣" (0.5s-1.0s)** ← *時間來自 Whisper，文字來自逐字稿* |
| "很好" (1.0s-1.5s) | "很好" | ✅ Match | **"很好" (1.0s-1.5s)** |

**實際案例：**
*   **Whisper**: "Broadcom **J** v6" (聽不懂 "Jay")
*   **Script**: "Broadcom **Jay** v6"
*   **Alignment**: 系統發現 "J" 和 "Jay" 在序列中的位置吻合，於是將 "J" 的時間戳 (例如 12.5s) 賦予給正確的 "Jay"。
*   **結果**：字幕顯示正確的 "Jay"，且時間精確同步。

**⚠️ 潛在風險與限制 (Limitations)**：

雖然此演算法非常強大，但在以下極端狀況下仍可能發生**對齊錯亂 (Misalignment)**：

1.  **逐字稿與語音嚴重不符**：
    *   如果逐字稿多了一整段「講者根本沒說的話」（例如稿中有備註 `(請看VCR)`，但語音沒有唸出來），演算法會嘗試強行把這些多餘的字塞進極短的時間內，導致前後字幕時間被擠壓變形。
    *   **解法**：務必確保逐字稿是**實際發音的內容**，移除括號備註。

2.  **重複語句或口吃**：
    *   語音：「我...我...我覺得」
    *   逐字稿：「我覺得」
    *   演算法可能會困惑要把「我」對齊到第一個還是第二個「我」，通常會對齊到第一個，導致字幕顯示稍早。

3.  **大段落的極速快語或含糊不清**：
    *   如果講者含糊不清連續唸了 20 個字，Whisper 可能只抓到 5 個字。這時演算法要在這 5 個時間錨點中去分配 20 個字的空間，可能會導致中間的字時間分配不均（忽快忽慢）。

---

### 🤔 深入解析：GPT 模型選擇 (4o-mini vs 4o/o1)

**目前的選擇：GPT-4o-mini**

**任務性質分析：**
這個步驟的任務是 **「語意斷句 (Semantic Segmentation)」**。
*   輸入：`今天天氣很好我們去公園玩吧`
*   輸出：`今天天氣很好\n我們去公園玩吧`

**是否需要升級到 GPT-4o 或 o1？**

| 模型 | 優點 | 缺點 | 適用性結論 |
| :--- | :--- | :--- | :--- |
| **GPT-4o-mini** | **極快**、極便宜、對簡單語意理解已足夠 (99% 準確率)。 | 處理極度複雜的巢狀從句可能偶爾誤判。 | ✅ **最佳選擇 (Current)**。對於「切分字幕」這種單純任務，mini 的性價比最高，速度也最快。 |
| **GPT-4o** | 理解力更強，能處理更隱晦的語氣停頓。 | 貴 10 倍以上，速度稍慢。 | ⚠️ **邊際效益遞減**。除非字幕包含大量雙關語或極度專業的學術長句，否則 mini 已足夠。 |
| **o1 (Reasoning)** | 擅長複雜推理 (如數學、Coding)。 | **極慢** (思考時間長)，非常貴。 | ❌ **不推薦**。斷句不需要「推理」，只需要「語感」。o1 會過度思考，導致處理時間暴增。 |

**建議：** 維持使用 **GPT-4o-mini**，若發現特定領域 (如醫療、法律) 斷句品質不佳，再考慮針對該領域切換至 **GPT-4o**。


---

### 階段二：高性能影片合成 (`batch_video_assembler.py`)

這個階段的目標是將靜態 Slide 與 Avatar 完美結合，確保畫面切換與聲音完全對上。

#### 1. 音訊指紋對齊 (Audio Fingerprinting Sync)
這是 V10 防止「搶拍」與「錯位」的核心技術。

*   **問題**：切片的 MP3 (如 `1.mp3`) 與 Avatar 總音軌的長度可能有微小差異（靜音、編碼差異）。如果直接按長度相加，誤差會累積。
*   **解法 (FFT Cross-Correlation)**：
    1.  讀取總音軌 (`Main Audio`) 與切片音軌 (`Segment Audio`)。
    2.  將兩者轉換為聲波訊號 (Waveform)。
    3.  使用 **FFT (快速傅立葉變換)** 計算相關性 (Correlation)。
    4.  **找出最大相關點**：即該切片在總音軌中「最吻合」的確切位置。
*   **結果**：我們不再「推算」第 5 張圖何時開始，而是「聽」出它確實在總音軌的第 32.54 秒出現。

#### 2. 幀級精確時長計算 (Frame-Perfect Duration)
為了消除累積誤差，我們放棄「秒」作為單位。

*   **邏輯**：
    *   計算 Slide A 的開始幀數：`Start_Frame_A = Round(Start_Time_A * 24)`
    *   計算 Slide B 的開始幀數：`Start_Frame_B = Round(Start_Time_B * 24)`
    *   **Slide A 持續長度 (幀)** = `Start_Frame_B - Start_Frame_A`
*   **作用**：數學上保證了 `Sum(所有片段幀數) == 總影片幀數`。
*   **修正搶拍**：若計算出的長度比 MP3 稍長，程式會自動填充靜音 (`-af apad`)；若稍短，則輕微修剪靜音。這確保了畫面永遠不會比聲音因為「四捨五入」而慢慢超前。

#### 3. 平行渲染 (Parallel Rendering)
*   **技術**：使用 `ThreadPoolExecutor` 開啟 8 條執行緒。
*   **操作**：同時呼叫 8 個 FFmpeg 實例，分別生成 `segment_1.mp4`, `segment_2.mp4`...
*   **效能**：將原本需要 30 分鐘的渲染過程壓縮至 3~5 分鐘。

#### 4. 最終合成 (Final Composition)
*   **圖層堆疊**：
    *   Layer 1 (底層): 串接好的 Slide 影片 (`base_track.mp4`)
    *   Layer 2 (上層): 經過圓形遮罩處理的 Avatar (`avatar_processed.mov`)
    *   Layer 3 (字幕): 燒錄 ASS 字幕
*   **音訊來源**：使用 `avatar_full.mp4` 的原始音軌。

---

## 總結：為何 V10 更好？

1.  **不再搶拍**：指紋對齊 + 幀級計算，消除了所有時間軸漂移的可能性。
2.  **不再影音不同步**：堅持使用 Avatar 原始音軌到底，不進行二次混音。
3.  **速度極快**：Whisper API + 平行運算，讓製作時間大幅縮短。
